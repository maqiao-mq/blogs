# 内存散装知识点记录

## munmap 和 madvise

munmap() 在调用的时候，会尽可能把不用的 page table 给释放掉。

madvise() 有两个flag：

- MADV_DONTNEED，表示用户态不再需要这块内存了，内核在处理的时候，会直接无视内存是否脏页而回收掉。如果后续又去访问这块内存的话，会触发 page fault，然后系统会重新分配一块内存出来。
- MADV_FREE，也和MADV_DONTNEED一样，直接回收，区别是它会lazy free，也就是并不会立即回收，而是在系统内存不够的时候再来回收。而且，期间如果又对这块内存进行了写入动作的话，就会取消回收。

madvise的两个flag，和munamp的区别，在于madvise()目前是不会主动释放page table的，但是貌似upstream有补丁在尝试增加回收page table的能力。

jemalloc和tcmalloc一般会先用 madvise 来回收内存，最后才会用 munmap() 来彻底回收。

# defrag

在 `/sys/kernel/mm/transparent_hugepages/`目录下有2个文件，一个是 `enabled`，一个是 `defrag`。

- `enabled`有3个选项：
  - always，总是尝试使用thp
  - madvise，只有在使用了 madvise 标记的vma，才尝试使用 thp
  - never，不启用 thp
- `defrag` 有类似的选项，但含义完全不同：
  - always，当在page fault一类的上下文分不出大页的时候，就同步等回收、整理，直到有大页为止
  - madvise，只有用madvise标记了的vma在分不出大页的时候，才会同步等，其他情况下fallback到用4k页
  - defer，默认直接后台做异步回收、整理，后续再对它进行替换，第一次分配的时候使用4k页
  - defer+madvise，对madvise标记的区域做always的动作，其他区域做defer的动作
  - never

二者在整个内存的架构体系中生效的位置是不一样的。

在 `handle_mm_fault`的时候，如果判断出现fault 的是匿名内存，且在这个区域可以使用大页的话，就会进入分配大页的路径尝试分配。

在判断是否可用大页的时候，会进入函数 `thp_vma_allowable_order()`，这里就进入到了 `enabled`的逻辑了。如果是配置了always，或者这个vma用madvise标记过，才会允许使用大页。

在进入分配大页的路径 `do_huge_pmd_anonymous_page（）`以后，内核会调用 `vma_alloc_folio()`（老版本是 `vma_alloc_page`)来分配大页，调用该函数需要传递gfp来控制buddy的行为，这个gfp取决于 `vma_thp_gfp_mask()`函数的返回值，里面会结合 `defrag` 的配置来做判断。

在 `vma_alloc_folio（)`里面，根据gfp的行为，可能同步等，可能异步等，但最终返回的结果，可能是拿到了大页，也可能是没拿到。拿到了大页的话，就会把大页给安到pmd上，没拿到就会fallback到用4k页。

`vma_thp_gfp_mask()`这里的逻辑有必要贴一下：

```c
/*
 * always: directly stall for all thp allocations
 * defer: wake kswapd and fail if not immediately available
 * defer+madvise: wake kswapd and directly stall for MADV_HUGEPAGE, otherwise
 *		  fail if not immediately available
 * madvise: directly stall for MADV_HUGEPAGE, otherwise fail if not immediately
 *	    available
 * never: never stall for any thp allocation
 */
gfp_t vma_thp_gfp_mask(struct vm_area_struct *vma)
{
	const bool vma_madvised = vma && (vma->vm_flags & VM_HUGEPAGE);

	/* Always do synchronous compaction */
	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))
		return GFP_TRANSHUGE | (vma_madvised ? 0 : __GFP_NORETRY);

	/* Kick kcompactd and fail quickly */
	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG, &transparent_hugepage_flags))
		return GFP_TRANSHUGE_LIGHT | __GFP_KSWAPD_RECLAIM;

	/* Synchronous compaction if madvised, otherwise kick kcompactd */
	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG, &transparent_hugepage_flags))
		return GFP_TRANSHUGE_LIGHT |
			(vma_madvised ? __GFP_DIRECT_RECLAIM :
					__GFP_KSWAPD_RECLAIM);

	/* Only do synchronous compaction if madvised */
	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG, &transparent_hugepage_flags))
		return GFP_TRANSHUGE_LIGHT |
		       (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);

	return GFP_TRANSHUGE_LIGHT;
}
```

总结起来是：

| defrag配置    | vma是否标记 | gfp                                           | 含义                           |
| ------------- | ----------- | --------------------------------------------- | ------------------------------ |
| always        | Y           | GFP_TRANSHUG                                  | 同步compaction                 |
| always        | N           | GFP_TRANSHUG<br />__GFP_NORETRY               |                                |
| defer         | N/A         | GFP_TRANSHUGE_LIGHT<br />__GFP_KSWAPD_RECLAIM | 唤醒kcompactd进行异步整理      |
| defer+madvise | Y           | GFP_TRANSHUGE_LIGHT<br />__GFP_DIRECT_RECLAIM | 同步compaction                 |
|               | N           | GFP_TRANSHUGE_LIGHT<br />__GFP_KSWAPD_RECLAIM | 唤醒kcompactd进行异步整理      |
| madvise       | Y           | GFP_TRANSHUGE_LIGHT<br />__GFP_KSWAPD_RECLAIM | 唤醒kcompactd进行异步整理      |
|               | N           | GFP_TRANSHUGE_LIGHT                           |                                |
| never         | N/A         | GFP_TRANSHUGE_LIGHT                           | 不触发任何整理，分不出来就失败 |

# khugepaged

内核会创建一个 khugepaged 线程，这个线程的逻辑是，不断地扫描mm，遍历并找到其中符合条件的vma，然后判断一个pmd页上的所有page是否可以合并成一个大页，如果可以的话就进行合并和替换。

如果中途有用户动了khugepaged的开关，那么就会将这个 kthread 给关掉。

并不是所有的mm结构体都会被khugepaged扫描，内核维护了一个名为khugepaged_scan的全局链表，里面每个entry 是一个mm_slot，slot里面存的就是mm的信息。

在以下情况下，内核会调用khugepaged_enter_vma() 尝试将 mm 加入到这个链表中：

- vma发生合并
- 调用mmap进行映射
- 匿名内存发生pmd一级的page fault时
- 调用madvise设置MADV_HUGEPAGE标志时

upstream正在引入mthp的功能，核心思想是允许大页的大小是可变的，这在arm上很有用，因为arm支持4k、16k和64k等页面。

# alloc_page的过程

这里会丢弃掉各种buddy系统，pcp之类的细节，只看一些大方向的。

alloc_page是个宏，底下对应的函数是alloc_pages_noprof。具体流程是这样的：

- 先获取当前进程的memory policy，然后根据policy分别去调用alloc_pages_preferred_many、__alloc_pages_node_noprof或者__alloc_pages_noprof。但这些路径，最后都会走到 `__alloc_pages_noprof`。
- 进入正式分配的流程。先掉 `get_page_from_freelist`从buddy里面摸一把，这次故意置上 `ALLOC_NOFRAGMENT`，不允许buddy系统拆分一个memblock，一般情况下都是能拿到符合条件的page的，这个时候直接返回，这个就是fast path。
- 第一次拿不到，那么就进入slow path 了。
  - 如果gfp允许唤醒kswapd的话，就会先上来唤醒所有的kswapd，做内存回收
  - 再试一把不带上 `ALLOC_NOFRAGMENT`的flag，调用 `get_page_from_freelist`再来摸一把。因为放宽了限制，所以也很有可能成功。
  - 如果要分配的order比较大（大于3），那就先做一把直接的内存整理，尝试搞出来一块order足够的memblock。`__alloc_pages_direct_compact`
  - 如果gfp允许唤醒kswapd的话，那就再唤醒一把kswapd，做内存回收，确保内存分配期间kswapd都在跑着
  - 带上 `ALLOC_CMA`的flag，允许分配CMA内存，然后再来摸一把。`get_page_from_freelist`
  - 清掉对node的限制（清除nodemask），然后重新选择一个node的zone，再来摸一把。`get_page_from_freelist`
  - 尝试做同步reclaim，相当于把kswapd的活拿过来直接干一遍 `__alloc_pages_direct_reclaim`。然后再调用 `get_page_from_freelist`摸一把
  - 尝试做同步整理，相当于把kcompactd的活拿过来直接干一遍 `__alloc_pages_direct_compact`。然后再调用 `get_page_from_freelist`摸一把
  - 判定是否可以重试，是的话继续尝试唤醒kswapd，分配cma内存等等操作
  - 走到最后一步，准备oom kill杀进程，`__alloc_pages_may_oom`。
  - 调用 `__alloc_pages_cpuset_fallback`再来摸一把。

# compaction和kswapd

这两个机制是有配合的，所以放在一起来记录。

在 `alloc_page()`的过程中，如果内存不够，就会走到 `__alloc_pages_slowpath()`里面，这个时候就会根据 gfp 来决定是不是要做回收和整理，以及是同步还是异步地做。

```
- __alloc_pages_slowpath
	- wake_all_kswapds
	- __alloc_pages_direct_reclaim
		- __perform_reclaim
			- try_to_free_pages
				- do_try_to_free_pages
					- shrink_zones
						- shrink_node
		- get_page_from_freelist
	- __alloc_pages_direct_compact
```

## kswapd

kswapd 线程是每个node 一个，node在内核里面使用 pg_data_t 来表示的，所以在内核里面会看到很多对它的引用。

kswapd的核心是回收，术语是reclaim，kswapd的逻辑如下：

- 在kswapd 线程没有被stop之前，不断重复 `kswapd_try_to_sleep()`和 `balance_pgdat()`
- `kswapd_try_to_sleep()`的逻辑是，在 kswapd 已经回收了足够的内存的情况下，唤醒 kcompactd 进行内存整理，然后陷入睡眠等待下次唤醒。如果内存不满足要求，就不会陷入睡眠。

  - kswapd 判断内存是否足够 `(pgdat_balanced())`的逻辑比较复杂。node的每个zone有三个水位线watermark，即min、low、high。`pgdat_balanced()`会对每个zone调用 `zone_watermark_ok_safe（）`来判断 zone 的空闲内存是否符合 high 水位线，但是对于 order 大于 0 的情况下，还得确保至少有一个符合这个 order 的完整 page。order的来源，是某个进程在分配不到内存，唤醒kswapd时传进来的它当时需要的内存的page order。
- `balance_pgdat()`负责真正地干活。它主要做的事情：

  - 让 lru 的 page 老化一波
  - 通过 lru 回收那些作为 cache 的页面
  - 通过 shrinker 链表回收各种东西，主要是 slab
  - 唤醒 kcompactd 来做一波整理

```
- kswapd
	- balance_pgdat
		- kswapd_age_node
		- kswapd_shrink_node
			- shrink_node
				- shrink_node_memcgs
					- shrink_lruvec
					- shrink_slab
		- wakeup_kcompactd
```

## kcompactd

kcompatcd 线程也是每个node一个。它会陷入一个无限循环，每次循环干两件事：

- 等待一段时间，如果期间被被其他线程唤醒，就开始整理整个node，然后陷入下次等待
- 如果等待超时，则根据compaction_proactiveness的配置判断是否要主动进行整理

整理的逻辑：

node的每个zone，有自己的min、low、high水位线配置。

在遇到需要整理的情况时，一定是有某个order的page分不出来了，所以整理的判定和order还有关联。kcompactd的order来自于wakeup它的进程传入，在其他alloc_page的上下文中，也可能触发直接整理，也就是direct reclaim，这个时候由alloc_page的上下文传入。

在整理node的时候，实际上是按照每个zone为单位进行整理的。在整理之前，需要先判定zone是不是符合要整理的条件，这个在函数 `compaction_suit_allocation_order()`中可以看到。判定方式是：

- 判定一：看当前zone的空闲内存，是否低于min，高的话就不做整理了（因为还够用），对应函数 `zone_watermark_ok()`。低的情况下走判定二。

  - 除了在kcompactd的上下文，在其他alloc_page的上下文中，也可能触发直接整理，也就是direct reclaim，这时判定的水位线可能是min、可能是low。
- 判定二：带上ALLOC_CMA（也就是运行从CMA拿内存），再根据order是否大于3来动态决定使用low（大于）还是min（小于）水位线，再调用 `__zone_watermark_ok()`来判断空闲内存是否高于水位线。

  - 在高于水位线的情况下，如果碎片化程度也较高（大于sysctl_extfrag_threshold），则需要整理
  - 在低于水位线的情况下，说明整理很可能没啥意义了，所以跳过整理

具体的整理过程，则是对这个zone，用两个指针，一个从头到尾，一个从尾到头进行扫描，找到合适的空洞以后把一遍占用的内存给move过去，同时利用rmap的记录修改掉所有的映射关系。流程实际上很复杂，但是网上资料较多，这里不关注了。

具体逻辑在 `compact_zone()`中。

```
- kcompactd
	- kcompactd_do_work
		- compaction_suit_allocation_order
		- compact_zone
	- should_proactive_compact_node
	- fragmentation_score_node
	- compact_node
```

主动进行整理的逻辑和被动整理不太一样，主动整理会判定当前node的碎片化程度是否已经超过了阈值sysctl_compaction_proactiveness，如果超过了就会调用 `compact_zone()`进行整理。同时，它还要避开kswapd，也就是如果这个node正在被kswapd回收，那就先不做整理。

# ksm

ksm的机制，是开一个全局唯一的内核线程 ksmd，它会在后台对候选的页进行扫描，如果发现有两页是相同的，就把他们合并成一个页，并改成write protect（方便后续触发cow）。

扫描的逻辑是用 2 个红黑树，一个是unstable，一个是 stable。 stable tree 存放的是已经改成了 write protect 的那些页，unstable 的是那些候补的，还可能被其他线程并发修改的页。 tree 其实没有一个确切的key，在遍历红黑树时，比较的逻辑就是用两个页的内容做 memcmp，小于往左，大于往右，相等就是同样的页。

系统上可用的页太多了，所以不可能把所有的页都加进来做比较，所以是有限制的。

- 一种是调用 madvise 把一段内存声明为 MADV_MERGEABLE，告诉内核这段内存应该被 ksm 扫描。
- 一种是调用 prctl + PR_SET_MEMORY_MERGE 将整个进程的内存都标记为可被ksm扫描。

被标记了以后，也只有符合特定条件的vma可以被合并。像hugetlb、shared、io相关的、dax之类的，都是不能被ksm合并的的。

```c
static bool vma_ksm_compatible(struct vm_area_struct *vma)
{
	if (vma->vm_flags & (VM_SHARED  | VM_MAYSHARE   | VM_PFNMAP  |
			     VM_IO      | VM_DONTEXPAND | VM_HUGETLB |
			     VM_MIXEDMAP| VM_DROPPABLE))
		return false;		/* just ignore the advice */

	if (vma_is_dax(vma))
		return false;

#ifdef VM_SAO
	if (vma->vm_flags & VM_SAO)
		return false;
#endif
#ifdef VM_SPARC_ADI
	if (vma->vm_flags & VM_SPARC_ADI)
		return false;
#endif

	return true;
}
```

此外，被加入扫描队列的mm，一般会在进程退出的时候，主动从队列中移除掉。

加入扫描队列对应函数：`__ksm_enter()`

退出扫描队列对应函数：`__ksm_exit`

# numa balance

numa balance是和内核的内存策略，cpu 调度相关联的。

它的原始需求很简单，就是一个进程本来已经在一个numa上跑了一段时间，分配了某些numa上的内存了，结果因为调度均衡的问题被换到了另一个numa上跑，或者由于内存策略、内存不足等原因分配了不合适的numa的内存，导致当前cpu运行这个进程的时候取numa内存延时很长，所以需要把这些已经分配的内存给换到更合适的numa内存上。

和以往的很多内存机制（比如khugepaged、ksmd、kswapd、kcompactd）一样，numa balance实际上也是一个周期性的，主动式的解决方案。但是和其他机制不一样的地方在于，numa balance没有自己的kthread，而是突出一个从各种用户态进程里面偷时间。

numa balance分为收集内存、迁移内存两步。

收集内存是在 cfs 的周期性 tick 中触发的，但不是这个时候做。简单来说，是在 `task_tick_fair()`的时候，调用 `task_tick_numa()`，这里的逻辑是，如果当前正在运行的进程不是kthread，也没有正在退出，而且离上次做numa balance扫描的时间有点长了，那么就扔一个 `numa_work` 到 `task_work`里面。

这里可以看到：

1. 内核并不会在tick这里做太多复杂的事情，要不然就耽误调度了
2. 只有cfs的tick才会触发，但并不代表只有cfs的进程才会被numa balance，rt的同样会。只是在cfs的tick里面放这个检查，不太会耽误rt进程的运行。

`task_work`机制是在进程回到用户态的时候，才运行这些work。所以很明显，numa balance的扫描实际上在进程的上下文偷时间。

接下来是在task_work里面做扫描的过程。

- 第一步同样是判断离上次扫描有多久了。时间太短的话就不做了，免得频率太高了。
- 然后就是遍历当前进程所有的vma，把符合条件的vma的页表的读写属性全改成不可读不可写，也就是PTE_NONE。这样做的目的，是后续进程访问这个页的时候，会触发page fault，然后在page fault里面做页面迁移。
- 要符合numa balance的vma是很严格的，比如mmio内存不行，dax内存不行，so映射的内存不行（天生就会被map后被各个numa的cpu访问）等等。

再往下就是等着进程访问内存的时候触发page fault了。在触发了以后，就会检查到页面是PTE_NONE的，但vma又标记自己是可访问的，这个时候内核就知道这个页是被numa balance改过的，所以就要准备做迁移了。这里的具体实现在 `do_numa_page()`里面。

这里也不是说就直接把页换走了，也要经过一堆的检查。其中比较重要的是：

- 确保当前页面的page，和当前vma的mempolicy要求的node节点是不是对应的上的，如果吻合，说明完全没有出现内存错配的情况，不需要换掉
- 如果当前page已经在file lru链表上了，而且还是so的代码段（判定方式是可执行权限+映射到了多个进程），那就没必要换掉了
- 如果当前page已经在file lru链表上了，而且还是个脏页，那也没必要换掉
- 如果新页所在的目标node，可用内存已经不够了（低于high watermark），那也没必要强行换了，不然后面还会因为内存不足的原因fallback到不合适的node上

在这些情况都通过检查后，才会对页面换掉。原理就是在合适的node上，分配一个page出来，然后把当前page的内容拷贝到新的page上，并且把这个page的pte映射关系都改成新的page，然后刷tlb。

当然，如果发现并不适合换掉，就会直接把page的pte属性给改回去，避免后续的访问又触发page fault。

# memory policy

由于numa的亲和性问题，如何让一个cpu上运行的进程能分配到它最合适的numa内存就是个问题了。为此内核抽象出了memory policy来控制这个事情。

这块内核的文档其实写的很清楚了，可以看[这个文档](https://docs.kernel.org/admin-guide/mm/numa_memory_policy.html)

这里简单记录下，方便日后快速捡起来。

## 层级

1. 系统默认policy。
   这个是系统默认的，硬编码的policy，倾向于在本地numa分配内存。
2. task 特定 policy
   如果 task 指定了policy，就会覆盖系统默认policy，没指定就直接用系统默认policy。这允许进程实现一些numa aware的优化。通过 `set_mempolicy()`这个系统调用可以设置task policy。
3. VMA 特定policy
   匿名内存可以用 `mbind()`来指定特定内存区域的policy。如果这段区域只是原来vma的一部分，那么这个vma很可能被分割成2-3个vma。vma policy是最高优先级，能覆盖 task policy。
4. shared policy
   这个是概念上的。简单来说，如果用 `mbind()`指定的区域，恰好是一个共享内存的，那么这个policy会被所有共享这段内存的进程使用。

## 策略

- MPOL_BIND，只能在指定的numa节点中分配内存（通过nodemask指定）
- MPOL_PREFERRED，优先在指定的numa节点中分配内存，不够的话可以到所有的numa中找
- MPOL_PREFERRED_MANY，优先在指定的一堆numa节点中分配内存，不够的话可以到所有的numa中找
- MPOL_INTERLEAVED，在指定的一堆numa中以round-robin的方式来交错分配。具体来说，每个进程会记录一个index，每次分配了以后更新index，以便指向下一个numa。
- MPOL_WEIGHTED_INTERLEAVE，和MPOL_INTERLEAVED差不多，但是给每个numa加入了权重。比如说，node [0,1]的权重分别是[5,2]，那前5次从node 0 分配，后2次从node 1分配，然后循环。

// TODO： 解释下这个sys_set_mempolicy_home_node

# lru

上面说到，kswapd会调用到 `shrink_lruvec`，这个地方做的事情就是遍历lru链表上，将一些页给淘汰掉。

虽然想要做的事情很简单，但是实际执行的时候却非常复杂，有很多的细节。

## lru的组成

* 每个cpu有一个folio_batch，是第一级缓存
* 每个memcg，会记录与每个node的关联情况，称为 `mem_cgroup_per_node`，每个里面有一个lruvec。所以每个memcg有N个lruvec，N为node的数量。在使用memcg的情况下，page最终会被刷到这里面来。
* 每个node的pglist_data里面，有一个lruvec。在不使用memcg的情况下，才会使用这个lruvec来做lru。

一般的流程是：

- page先被放到percpu的folio_batch中
- 在各种时机会调用 `lru_add_drain`将它刷到memcg或者node的lruvec中
- kswapd调用 `shrink_node`，或者内核申请内存的路径直接调用 `shrink_node`，这个时候会遍历每个memcg的lruvec，调用 `shrink_lruvec`来处理每个lru链表上的page，把他们淘汰掉，或者重新放到active里面去。

lru的分类也比较简单，共5中：

1. active、inactive的匿名页
2. active、inactive的文件页
3. unevictable的页


加入时机：

在处理page fault的上下文，即 `handle_mm_fault()`中，会处理各种类型的page fault。在各种page fault的处理中，如果page符合条件，就会调用 `folio_add_lru_vma()`或者 `lru_add_drain()`将page加入到percpu的folio_batch中进行管理。比如：

- `wp_page_copy`，cow的新页面
- `do_swap_page`，从swap分区读上来的内存
- `do_anonymous_page`，匿名vma的映射
- ...


## lru 淘汰页面

`shrink_folio_list`

# swap设备

我们可以通过swapon来将一个文件作为swap分区，对内存扩容。问题是，内核会在什么时候把页面写进去，什么时候把页面读出来。

swap是由两块组成的，swap cache 和 swap file。

流程主要是这样的：

1. 当 `shrink_folio_list`判定一个page可以被淘汰到swap分区的时候，就会调用 `try_to_unmap_one`来准备改掉pte，取消映射，然后调用 `add_to_swap`将page加入到swap cache中。
2. 这里 `add_to_swap`的逻辑是找注册给系统的那些swap file中，哪个file的哪块区域是空的（对应的术语是 `swp_entry`），同时还会将对应的page放到该file的address_space的xarray上去。
3. `shrink_folio_list`后续的处理逻辑中，会调用 `pageout`来将页换出去，`pageout`实际上会调用 `mapping->a_ops->writepage`，走到 `swap_writepage`，将page刷到盘上
4. 在后续触发了page fault时，会在 `handle_mm_fault`的路径上，一路往下走到 `do_swap_page`上。这个时候会根据pte的值来反向找到swap file，再找回到对应的address_space。如果在address_space上发现page还在，则说明还没来得及写下去，或者被预读上来了，这个时候直接填回去就行。否则，就要从盘上再读上来。当然，这个被重新踩中的page，还会被从swap cache中移除( `folio_free_swap`)，因为它被踩中说明很需要，不应该再被换出去了。

剩下的一个事情是，他们是什么时候被判定成可以被swap的。

当内核发生page fault时，分配一块page来填页表的时候，如果这块页是匿名页，那么就会调用 `folio_add_new_anon_rmap()`，这个时候就会使用 `__folio_set_swapbacked()`来标记它可以被换出到swap分区。

# working set

这个是为了解决内存颠簸问题，在lru的基础上增加的额外的一层。
